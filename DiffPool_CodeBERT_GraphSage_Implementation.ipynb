{"cells":[{"cell_type":"markdown","metadata":{"id":"gbDron_rv4FZ"},"source":["# Args"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YuQSONW4v7Fw"},"outputs":[],"source":["args = {\n","        'device': 0,\n","        'num_layer': 3,\n","        'emb_dim': 256,\n","        'drop_ratio': 0,\n","        'batch_size': 64,\n","        'lr': 0.01,\n","        'epochs': 25,\n","        'num_vocab': 5000,\n","        'max_seq_len': 5,\n","        'diff_pool_layers': [(32, 3)],\n","        'max_num_nodes': 1500,\n","        'random_split': False,\n","        'dataset': \"ogbg-code2\",\n","        'num_workers': 0,\n","        'best_model_path': \"/content/gdrive/MyDrive/CS MS/Colab_Notebooks/CS224W_Final_Project/best_model_params.pt\",\n","        'model_path': \"/content/gdrive/MyDrive/CS MS/Colab_Notebooks/CS224W_Final_Project/model_params.pt\",\n","        'eval_results_path': '/content/gdrive/MyDrive/CS MS/Colab_Notebooks/CS224W_Final_Project/eval_results.pt',\n","        'checkpoint': 5, # PLEASE MODIFY BASED ON EPOCH INTERVAL YOU WANT TO UPDATE THE MODEL...\n","        'is_codeBERT_encoder': False,\n","    }\n","\n","if args['is_codeBERT_encoder']:\n","    args['emb_dim'] = 768\n"]},{"cell_type":"markdown","metadata":{"id":"ReI_oxpyvp9a"},"source":["# Mount drive\n","\n","Please ensure you have placed the shortcut to the folder in the appropriate location, namely the CS224W_Final_Project folder in your Colab Notebooks folder that's auto-generated."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17939,"status":"ok","timestamp":1702821511191,"user":{"displayName":"Nathanael Ren","userId":"05350445776639853397"},"user_tz":300},"id":"vrB0yqOmeiqN","outputId":"eeecdcbd-873f-4579-a4ad-6faa4d0be35b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2136,"status":"ok","timestamp":1702821513326,"user":{"displayName":"Nathanael Ren","userId":"05350445776639853397"},"user_tz":300},"id":"HrZBJl7_e_hl","outputId":"e332c8e7-cb7c-4421-a06a-f0c6095e659e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/gdrive/.shortcut-targets-by-id/1TP2U8QE_WOrmvdLAUnAS09n-2t-Wp9Z_/CS224W_Final_Project/OGB_submission\n"," best_model_ast_gs_params.pt\n"," best_model_codeB_gs_params.pt\n"," best_model_graphSage_params.pt\n"," dataset\n"," eval_ast_gs_results.pt\n"," model_codeB_gs_params.pt\n"," model_params.pt\n","'OGB Submission: DiffPool_AST_GraphSage [(32,3)].ipynb'\n","'OGB Submission: DiffPool_CodeBERT_GraphSage [(32,3)]_256_dim.ipynb'\n","'OGB Submission: DiffPool_CodeBERT_GraphSage [(32,3)].ipynb'\n"," vocab_list.pt\n"]}],"source":["%cd /content/gdrive/MyDrive/CS\\ MS/Colab_Notebooks/CS224W_Final_Project\n","! ls # verify that you are in the right directory"]},{"cell_type":"markdown","metadata":{"id":"xLMap4xcvGwG"},"source":["# Setting up the dependencies and download the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47779,"status":"ok","timestamp":1702821561103,"user":{"displayName":"Nathanael Ren","userId":"05350445776639853397"},"user_tz":300},"id":"70NcYDLQ6rop","outputId":"273b8584-dacc-41d2-d620-641b6cd3f129"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ogb\n","  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.5)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n","Collecting outdated>=0.2.0 (from ogb)\n","  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n","Collecting littleutils (from outdated>=0.2.0->ogb)\n","  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n","Building wheels for collected packages: littleutils\n","  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=96904c4105336c9820f025d54cb2d4af4c28b36f96d227bea6ddec1ddff93ca1\n","  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n","Successfully built littleutils\n","Installing collected packages: littleutils, outdated, ogb\n","Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n","Collecting torch_geometric\n","  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.4.0\n","1.3.6\n","2.1.0+cu121\n","Device: cuda\n"]}],"source":["!pip install ogb\n","!pip install torch_geometric\n","!python -c \"import ogb; print(ogb.__version__)\"\n","\n","import os\n","import os.path\n","from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n","from torch_geometric.loader import DataLoader\n","import torch\n","import pandas as pd\n","import torch.nn.functional as F\n","from tqdm.notebook import tqdm\n","print(torch.__version__)\n","\n","from torch_geometric.nn.dense import DenseSAGEConv\n","\n","import torch_geometric.transforms as T\n","from torch_geometric.nn import global_add_pool, global_mean_pool\n","from numpy import float32\n","from transformers import AutoTokenizer, AutoModel\n","\n","from copy import deepcopy\n","\n","dataset = PygGraphPropPredDataset(name = \"ogbg-code2\")\n","import copy\n","import numpy as np\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Device: {}'.format(device))"]},{"cell_type":"markdown","metadata":{"id":"6bo9YEJuQ6P4"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxo9TsBKQ8jy"},"outputs":[],"source":["from collections import Counter\n","import numpy as np\n","import torch\n","\n","class SemanticNodeEncoder(torch.nn.Module):\n","    '''\n","    Use CodeBERT to convert node type strings and node attributes strings into embeddings.\n","    Depth is not used in this embedding.\n","        Input:\n","            emb_dim: default node feature of N X D\n","            node_types: list of node type strings [98 type_string]\n","            node_attributes: list of node attribute strings [10029 attribute_string]\n","            depth: The depth of the node in the AST.\n","        Output:\n","            BERTCode-based embedding of node attribute strings. Dim: N x 768\n","    '''\n","    def __init__(self, codeBert, tokenizer, emb_dim, node_type_mapping, node_attributes_mapping):\n","        super(SemanticNodeEncoder, self).__init__()\n","\n","        self.max_depth = 20\n","        self.codeBert = codeBert\n","        self.tokenizer = tokenizer\n","        self.emb_dim = emb_dim\n","        self.node_type_mapping = node_type_mapping\n","        self.node_attributes_mapping = node_attributes_mapping\n","\n","        self.type_encoder = torch.nn.Embedding(len(node_type_mapping), emb_dim)\n","        self.attribute_encoder = torch.nn.Embedding(len(node_attributes_mapping), emb_dim) # 10029 x 768\n","        self.depth_encoder = torch.nn.Embedding(self.max_depth+1, emb_dim)\n","\n","        #self.type_encoder.weight = torch.nn.Parameter(self.get_embedding(node_type_mapping))\n","        self.attribute_encoder.weight = torch.nn.Parameter(self.get_embedding(node_attributes_mapping))\n","\n","    def get_embedding(self, mapping):\n","        '''\n","        Input:\n","            mapping: Either list of node type strings [98 type_string] or list of node attribute strings [10029 attribute_string]\n","        Output:\n","            BERTCode-based embedding of node attribute strings. Dim: N x 768\n","        '''\n","        node_embeddings = []\n","        feature_embedded = {}\n","        for i, feature_string in enumerate(mapping):\n","            if feature_string in feature_embedded:\n","                node_embedding = feature_embedded[feature_string]\n","            else:\n","                if i in [0, 1, 10021, 10022, 10023, 10024, 10027, 10028, 10029]: # some node attr are not recognized\n","                    feature_string = '[UNK]'\n","\n","                # standard codeBert steps taken to transform strings into embeddings\n","                try:\n","                    tokens = self.tokenizer.tokenize(feature_string)\n","                    tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","                    tokens_tensor = torch.tensor(tokens_ids).to(device)[None,:]\n","                    node_embedding = self.codeBert(tokens_tensor)[0].view(tokens_tensor.shape[1], -1)\n","                    node_embedding = node_embedding.sum(dim=0, keepdim=True) # sum embeddings if >1 word in the feature string\n","                    #print(node_embedding.shape)\n","                except:\n","                    print(\"Error tokenizing {} {}\".format(feature_string, i))\n","\n","                feature_embedded[feature_string] = node_embedding\n","            node_embeddings.append(node_embedding)\n","\n","        node_embeddings = torch.cat(node_embeddings, dim=0) # Nx768\n","        #print(node_embeddings.shape)\n","        return node_embeddings\n","\n","    def forward(self, x, depth):\n","        '''\n","            Input:\n","                'x': default node feature. the first and second column represents node type and node attributes.\n","                'depth': The depth of the node in the AST.\n","            Output:\n","        '''\n","        depth[depth > self.max_depth] = self.max_depth # 1xN of depth 1-20\n","        # print(x[:, 0]) # Nx1, holding the id of the node type\n","\n","        return self.type_encoder(x[:,0]) + self.attribute_encoder(x[:,1]) + self.depth_encoder(depth)\n","\n","class ASTNodeEncoder(torch.nn.Module):\n","    '''\n","        Input:\n","            x: default node feature. the first and second column represents node type and node attributes.\n","            depth: The depth of the node in the AST.\n","\n","        Output:\n","            emb_dim-dimensional vector\n","\n","    '''\n","    def __init__(self, emb_dim, num_nodetypes, num_nodeattributes, max_depth):\n","        super(ASTNodeEncoder, self).__init__()\n","\n","        self.max_depth = max_depth\n","\n","        self.type_encoder = torch.nn.Embedding(num_nodetypes, emb_dim)\n","        self.attribute_encoder = torch.nn.Embedding(num_nodeattributes, emb_dim)\n","        self.depth_encoder = torch.nn.Embedding(self.max_depth + 1, emb_dim)\n","\n","\n","    def forward(self, x, depth):\n","        depth[depth > self.max_depth] = self.max_depth\n","        return self.type_encoder(x[:,0]) + self.attribute_encoder(x[:,1]) + self.depth_encoder(depth)\n","\n","\n","\n","def get_vocab_mapping(seq_list, num_vocab):\n","    '''\n","        Input:\n","            seq_list: a list of sequences\n","            num_vocab: vocabulary size\n","        Output:\n","            vocab2idx:\n","                A dictionary that maps vocabulary into integer index.\n","                Additioanlly, we also index '__UNK__' and '__EOS__'\n","                '__UNK__' : out-of-vocabulary term\n","                '__EOS__' : end-of-sentence\n","\n","            idx2vocab:\n","                A list that maps idx to actual vocabulary.\n","\n","    '''\n","\n","    vocab_cnt = {}\n","    vocab_list = []\n","    for seq in seq_list:\n","        for w in seq:\n","            if w in vocab_cnt:\n","                vocab_cnt[w] += 1\n","            else:\n","                vocab_cnt[w] = 1\n","                vocab_list.append(w)\n","\n","    cnt_list = np.array([vocab_cnt[w] for w in vocab_list])\n","    topvocab = np.argsort(-cnt_list, kind = 'stable')[:num_vocab]\n","\n","    print('Coverage of top {} vocabulary:'.format(num_vocab))\n","    print(float(np.sum(cnt_list[topvocab]))/np.sum(cnt_list))\n","\n","    vocab2idx = {vocab_list[vocab_idx]: idx for idx, vocab_idx in enumerate(topvocab)}\n","    idx2vocab = [vocab_list[vocab_idx] for vocab_idx in topvocab]\n","\n","    # print(topvocab)\n","    # print([vocab_list[v] for v in topvocab[:10]])\n","    # print([vocab_list[v] for v in topvocab[-10:]])\n","\n","    vocab2idx['__UNK__'] = num_vocab\n","    idx2vocab.append('__UNK__')\n","\n","    vocab2idx['__EOS__'] = num_vocab + 1\n","    idx2vocab.append('__EOS__')\n","\n","    # test the correspondence between vocab2idx and idx2vocab\n","    for idx, vocab in enumerate(idx2vocab):\n","        assert(idx == vocab2idx[vocab])\n","\n","    # test that the idx of '__EOS__' is len(idx2vocab) - 1.\n","    # This fact will be used in decode_arr_to_seq, when finding __EOS__\n","    assert(vocab2idx['__EOS__'] == len(idx2vocab) - 1)\n","\n","    return vocab2idx, idx2vocab\n","\n","def augment_edge(data):\n","    '''\n","        Input:\n","            data: PyG data object\n","        Output:\n","            data (edges are augmented in the following ways):\n","                data.edge_index: Added next-token edge. The inverse edges were also added.\n","                data.edge_attr (torch.Long):\n","                    data.edge_attr[:,0]: whether it is AST edge (0) for next-token edge (1)\n","                    data.edge_attr[:,1]: whether it is original direction (0) or inverse direction (1)\n","    '''\n","\n","    ##### AST edge\n","    edge_index_ast = data.edge_index\n","    edge_attr_ast = torch.zeros((edge_index_ast.size(1), 2))\n","\n","    ##### Inverse AST edge\n","    edge_index_ast_inverse = torch.stack([edge_index_ast[1], edge_index_ast[0]], dim = 0)\n","    edge_attr_ast_inverse = torch.cat([torch.zeros(edge_index_ast_inverse.size(1), 1), torch.ones(edge_index_ast_inverse.size(1), 1)], dim = 1)\n","\n","\n","    ##### Next-token edge\n","\n","    ## Obtain attributed nodes and get their indices in dfs order\n","    # attributed_node_idx = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n","    # attributed_node_idx_in_dfs_order = attributed_node_idx[torch.argsort(data.node_dfs_order[attributed_node_idx].view(-1,))]\n","\n","    ## Since the nodes are already sorted in dfs ordering in our case, we can just do the following.\n","    attributed_node_idx_in_dfs_order = torch.where(data.node_is_attributed.view(-1,) == 1)[0]\n","\n","    ## build next token edge\n","    # Given: attributed_node_idx_in_dfs_order\n","    #        [1, 3, 4, 5, 8, 9, 12]\n","    # Output:\n","    #    [[1, 3, 4, 5, 8, 9]\n","    #     [3, 4, 5, 8, 9, 12]\n","    edge_index_nextoken = torch.stack([attributed_node_idx_in_dfs_order[:-1], attributed_node_idx_in_dfs_order[1:]], dim = 0)\n","    edge_attr_nextoken = torch.cat([torch.ones(edge_index_nextoken.size(1), 1), torch.zeros(edge_index_nextoken.size(1), 1)], dim = 1)\n","\n","\n","    ##### Inverse next-token edge\n","    edge_index_nextoken_inverse = torch.stack([edge_index_nextoken[1], edge_index_nextoken[0]], dim = 0)\n","    edge_attr_nextoken_inverse = torch.ones((edge_index_nextoken.size(1), 2))\n","\n","\n","    data.edge_index = torch.cat([edge_index_ast, edge_index_ast_inverse, edge_index_nextoken, edge_index_nextoken_inverse], dim = 1)\n","    data.edge_attr = torch.cat([edge_attr_ast,   edge_attr_ast_inverse, edge_attr_nextoken,  edge_attr_nextoken_inverse], dim = 0)\n","\n","    return data\n","\n","def encode_y_to_arr(data, vocab2idx, max_seq_len):\n","    '''\n","    Input:\n","        data: PyG graph object\n","        output: add y_arr to data\n","    '''\n","\n","    # PyG >= 1.5.0\n","    seq = data.y\n","\n","    # PyG = 1.4.3\n","    # seq = data.y[0]\n","\n","    data.y_arr = encode_seq_to_arr(seq, vocab2idx, max_seq_len)\n","\n","    return data\n","\n","def encode_seq_to_arr(seq, vocab2idx, max_seq_len):\n","    '''\n","    Input:\n","        seq: A list of words\n","        output: add y_arr (torch.Tensor)\n","    '''\n","\n","    augmented_seq = seq[:max_seq_len] + ['__EOS__'] * max(0, max_seq_len - len(seq))\n","    return torch.tensor([[vocab2idx[w] if w in vocab2idx else vocab2idx['__UNK__'] for w in augmented_seq]], dtype = torch.long)\n","\n","\n","def decode_arr_to_seq(arr, idx2vocab):\n","    '''\n","        Input: torch 1d array: y_arr\n","        Output: a sequence of words.\n","    '''\n","\n","\n","    eos_idx_list = torch.nonzero(arr == len(idx2vocab) - 1, as_tuple=False) # find the position of __EOS__ (the last vocab in idx2vocab)\n","    if len(eos_idx_list) > 0:\n","        clippted_arr = arr[: torch.min(eos_idx_list)] # find the smallest __EOS__\n","    else:\n","        clippted_arr = arr\n","\n","    return list(map(lambda x: idx2vocab[x], clippted_arr.cpu()))\n"]},{"cell_type":"markdown","metadata":{"id":"8aQf6gJSv9lx"},"source":["# Define GNN and DiffPool"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWe_CB0ywFN4"},"outputs":[],"source":["from torch_geometric.nn import dense_diff_pool, global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set, DenseGCNConv\n","from torch_geometric.utils import to_dense_batch, to_dense_adj\n","import torch.nn.functional as F\n","\n","# The generic GNN used by different layers of DiffPool\n","class GraphSage(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n","        super(GraphSage, self).__init__()\n","\n","        # GNN layers\n","        self.convs = torch.nn.ModuleList()\n","        self.convs.extend([DenseSAGEConv(hidden_dim, hidden_dim) for i in range(num_layers-1)])\n","        self.convs.append(DenseSAGEConv(hidden_dim, output_dim))\n","\n","        # Batch norm\n","        self.bns = torch.nn.ModuleList()\n","        self.bns.extend([torch.nn.BatchNorm1d(hidden_dim) for i in range(num_layers-1)])\n","\n","        # Log Softmax\n","        self.softmax = torch.nn.LogSoftmax(dim=1)\n","\n","        # Probability of an element getting zeroed\n","        self.dropout = dropout\n","\n","    def forward(self, x, adj):\n","        for i in range(len(self.convs)-1):\n","          x = self.convs[i](x, adj)\n","          x = self.bns[i](x.transpose(1,2))\n","          x = x.transpose(1,2)\n","          x = F.relu(x)\n","          x = F.dropout(x, p=self.dropout, training=self.training)\n","        out = self.convs[-1](x, adj)\n","\n","        return out\n","\n","# A DiffPool layer that takes in a graph and pool the nodes to produce a\n","# smaller graph\n","class DiffPool(torch.nn.Module):\n","    def __init__(self, gnn_embed, gnn_pool, number_of_nodes_out):\n","        super(DiffPool, self).__init__()\n","        self.gnn_embed = gnn_embed\n","        self.gnn_pool = gnn_pool\n","        self.number_of_nodes_out = number_of_nodes_out\n","\n","    def forward(self, x, adj, mask=None):\n","        # Compute embeddings and assignment matrix with GNNs.\n","        z = self.gnn_embed(x, adj)\n","        s = self.gnn_pool(x, adj)\n","\n","        # Use the DiffPool to get pooled graph.\n","        x, adj, l1, e1 = dense_diff_pool(z, adj, s, mask)\n","\n","        return x, adj\n","\n","\n","\n","# The top-level abstraction for a composition of networks that takes in a batch\n","# of data and proudces a prediction\n","class GNN(torch.nn.Module):\n","    def __init__(self, num_vocab, max_seq_len, node_encoder, gnn_num_layers = 3, diff_pool_node_number_list=[(50, 3), (10, 3), (3, 3)], emb_dim = 300, drop_ratio = 0.5, graph_pooling = \"mean\", max_num_nodes=1000):\n","        super(GNN, self).__init__()\n","        self.drop_ratio = drop_ratio\n","        self.emb_dim = emb_dim\n","        self.num_vocab = num_vocab\n","        self.max_seq_len = max_seq_len\n","        self.graph_pooling = graph_pooling\n","        self.dropout_ratio = drop_ratio\n","        self.max_num_nodes = max_num_nodes\n","\n","        if gnn_num_layers < 2:\n","            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n","\n","\n","        self.node_encoder = node_encoder\n","\n","        # Define the DiffPool layers\n","        self.diff_pool_layers = torch.nn.ModuleList()\n","        for number_of_nodes_out, gnn_layer_num in diff_pool_node_number_list:\n","            gnn_embed = GraphSage(input_dim=emb_dim, hidden_dim=emb_dim, output_dim=emb_dim, num_layers=gnn_layer_num, dropout=drop_ratio)\n","            gnn_pool = GraphSage(input_dim=emb_dim, hidden_dim=emb_dim, output_dim=number_of_nodes_out, num_layers=gnn_layer_num, dropout=drop_ratio)\n","            diff_pool_layer = DiffPool(gnn_embed=gnn_embed, gnn_pool=gnn_pool, number_of_nodes_out=number_of_nodes_out)\n","            self.diff_pool_layers.append(diff_pool_layer)\n","\n","        # Define the final GNN that takes the output of the final\n","        self.final_gnn = GraphSage(input_dim=emb_dim, hidden_dim=emb_dim, output_dim=emb_dim, num_layers=gnn_num_layers, dropout=drop_ratio)\n","\n","        # Pooling function to generate whole-graph embeddings\n","        if self.graph_pooling == \"sum\":\n","            self.pool = global_add_pool\n","        elif self.graph_pooling == \"mean\":\n","            self.pool = global_mean_pool\n","        elif self.graph_pooling == \"max\":\n","            self.pool = global_max_pool\n","        elif self.graph_pooling == \"attention\":\n","            self.pool = GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, 1)))\n","        elif self.graph_pooling == \"set2set\":\n","            self.pool = Set2Set(emb_dim, processing_steps = 2)\n","        else:\n","            raise ValueError(\"Invalid graph pooling type.\")\n","\n","        self.graph_pred_linear_list = torch.nn.ModuleList()\n","\n","        if graph_pooling == \"set2set\":\n","            for i in range(max_seq_len):\n","                 self.graph_pred_linear_list.append(torch.nn.Linear(2*emb_dim, self.num_vocab))\n","\n","        else:\n","            for i in range(max_seq_len):\n","                 self.graph_pred_linear_list.append(torch.nn.Linear(emb_dim, self.num_vocab))\n","\n","    def forward(self, batched_data):\n","        x, edge_index, edge_attr, node_depth, batch = batched_data.x, batched_data.edge_index, batched_data.edge_attr, batched_data.node_depth, batched_data.batch\n","        # Embed the nodes of the AST\n","        x = self.node_encoder(x, node_depth.view(-1,))\n","        # DiffPool Layers\n","        z, mask = to_dense_batch(x, batch, max_num_nodes=self.max_num_nodes)\n","        s = to_dense_adj(edge_index, batch, max_num_nodes=self.max_num_nodes)\n","        for layer in range(len(self.diff_pool_layers)):\n","            if (mask is not None):\n","              z, s = self.diff_pool_layers[layer](z, s, mask)\n","              mask = None\n","            else:\n","              z, s = self.diff_pool_layers[layer](z, s)\n","\n","        # DiffPool Layeres without node_num limit\n","        # z, mask = to_dense_batch(x, batch)\n","        # s = to_dense_adj(edge_index, batch)\n","        # for layer in range(len(self.diff_pool_layers)):\n","        #     z, s = self.diff_pool_layers[layer](z, s)\n","\n","        # Final GNN and pooling\n","        x = self.final_gnn(z, s)\n","        del z, s, mask\n","        graph_emb = self.pool(x, batch=None)\n","\n","        pred_list = []\n","\n","        for i in range(self.max_seq_len):\n","            pred_list.append(self.graph_pred_linear_list[i](graph_emb))\n","\n","        return pred_list\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8FPK09rhzaIi"},"source":["# Main Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGE0z5mYzfze"},"outputs":[],"source":["import numpy as np\n","import argparse\n","from torchvision import transforms\n","import torch.optim as optim\n","\n","multicls_criterion = torch.nn.CrossEntropyLoss()\n","\n","def train(model, device, loader, optimizer):\n","    model.train()\n","\n","    loss_accum = 0\n","    iter = 0\n","    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n","        batch = batch.to(device)\n","        iter += 1\n","\n","        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n","            pass\n","        else:\n","            optimizer.zero_grad()\n","            pred_list = model(batch)\n","            # if (iter % 100 == 1):\n","            #     print(\"after pred\", iter, \" memory use:\", torch.cuda.memory_allocated() / 1e6, \"MB\")\n","\n","            loss = 0\n","            for i in range(len(pred_list)):\n","                loss += multicls_criterion(pred_list[i].to(torch.float32), batch.y_arr[:,i])\n","\n","            loss = loss / len(pred_list)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            # loss_accum += loss.item()\n","            loss_accum += float(loss)\n","            del pred_list\n","            del loss\n","        # break # TODO: REMOVE\n","\n","    print('Average training loss: {}'.format(loss_accum / (step + 1)))\n","\n","\n","def eval(model, device, loader, evaluator, arr_to_seq):\n","    '''\n","        Input: Takes in the GNN model, the training, validation or test set, and helper functions\n","        Returns: Dict of precision, recall and f1 scores in float\n","    '''\n","    model.eval()\n","    seq_ref_list = []\n","    seq_pred_list = []\n","\n","    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n","        batch = batch.to(device)\n","\n","        if batch.x.shape[0] == 1:\n","            pass\n","        else:\n","            with torch.no_grad():\n","                pred_list = model(batch)\n","\n","            mat = []\n","            for i in range(len(pred_list)):\n","                mat.append(torch.argmax(pred_list[i], dim = 1).view(-1,1))\n","            mat = torch.cat(mat, dim = 1)\n","\n","            seq_pred = [arr_to_seq(arr) for arr in mat]\n","\n","            # PyG = 1.4.3\n","            # seq_ref = [batch.y[i][0] for i in range(len(batch.y))]\n","\n","            # PyG >= 1.5.0\n","            seq_ref = [batch.y[i] for i in range(len(batch.y))]\n","\n","            seq_ref_list.extend(seq_ref)\n","            seq_pred_list.extend(seq_pred)\n","        # break # TODO: REMOVE\n","\n","    input_dict = {\"seq_ref\": seq_ref_list, \"seq_pred\": seq_pred_list}\n","\n","    return evaluator.eval(input_dict)\n","\n","def main(load_model):\n","    '''\n","    Input:\n","        Load_model: \"BEST\" to load best model, \"CHECKPT\" to load checkpoint aka\n","            where you stopped training, or \"OVERWRITE\" for overwriting everything\n","    '''\n","    print (\"Starting Training\")\n","    # Training settings\n","    print(args)\n","\n","    device = torch.device(\"cuda:\" + str(args['device'])) if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    seq_len_list = np.array([len(seq) for seq in dataset.data.y])\n","    print('Target sequence less or equal to {} is {}%.'.format(args['max_seq_len'], np.sum(seq_len_list <= args['max_seq_len']) / len(seq_len_list)))\n","\n","    split_idx = dataset.get_idx_split()\n","    num_train, num_valid, num_test = len(split_idx['train']), len(split_idx['valid']), len(split_idx['test'])\n","\n","    if args['random_split']:\n","        print('Using random split')\n","        perm = torch.randperm(len(dataset))\n","        split_idx['train'] = perm[:num_train]\n","        split_idx['valid'] = perm[num_train:num_train+num_valid]\n","        split_idx['test'] = perm[num_train+num_valid:]\n","\n","        assert(len(split_idx['train']) == num_train)\n","        assert(len(split_idx['valid']) == num_valid)\n","        assert(len(split_idx['test']) == num_test)\n","\n","    #shrink the train set\n","    # split_idx['train'] = split_idx['train'][:len(split_idx['train'])//10]\n","    # print(len(split_idx['train']))\n","\n","    ### building vocabulary for sequence predition. Only use training data.\n","    vocab2idx, idx2vocab = get_vocab_mapping([data.y for data in dataset], args['num_vocab'])\n","\n","    ### set the transform function\n","    # augment_edge: add next-token edge as well as inverse edges. add edge attributes.\n","    # encode_y_to_arr: add y_arr to PyG data object, indicating the array representation of a sequence.\n","    dataset.transform = transforms.Compose([augment_edge, lambda data: encode_y_to_arr(data, vocab2idx, args['max_seq_len'])])\n","\n","    ### automatic evaluator. takes dataset name as input\n","    evaluator = Evaluator(args['dataset'])\n","\n","    train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args['batch_size'], shuffle=True, num_workers = args['num_workers'])\n","    valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args['batch_size'], shuffle=False, num_workers = args['num_workers'])\n","    test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args['batch_size'], shuffle=False, num_workers = args['num_workers'])\n","\n","    nodetypes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'typeidx2type.csv.gz'))\n","    nodeattributes_mapping = pd.read_csv(os.path.join(dataset.root, 'mapping', 'attridx2attr.csv.gz'))\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n","    graphBert = AutoModel.from_pretrained(\"microsoft/graphcodebert-base\")\n","    graphBert.to(device)\n","\n","    ### Encoding node features into emb_dim vectors.\n","    ### The following three node features are used.\n","    # 1. node type\n","    # 2. node attribute\n","    # 3. node depth\n","    node_encoder = ASTNodeEncoder(args['emb_dim'], num_nodetypes = len(nodetypes_mapping['type']), num_nodeattributes = len(nodeattributes_mapping['attr']), max_depth = 20)\n","\n","    model = GNN(num_vocab = len(vocab2idx), max_seq_len = args['max_seq_len'],\n","                    node_encoder = node_encoder,\n","                    gnn_num_layers = args['num_layer'],\n","                    diff_pool_node_number_list = args['diff_pool_layers'],\n","                    emb_dim = args['emb_dim'],\n","                    drop_ratio = args['drop_ratio'],\n","                    graph_pooling = \"sum\",\n","                    max_num_nodes=args['max_num_nodes']).to(device)\n","\n","    param_size = 0\n","    for param in model.parameters():\n","        param_size += param.nelement() * param.element_size()\n","    buffer_size = 0\n","    for buffer in model.buffers():\n","        buffer_size += buffer.nelement() * buffer.element_size()\n","\n","    size_all_mb = (param_size + buffer_size) / 1024**2\n","    print('model size: {:.3f}MB'.format(size_all_mb))\n","\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    num_params = sum(p.numel() for p in model.parameters())\n","    print(f'#Params: {num_params}')\n","\n","    valid_curve = []\n","    test_curve = []\n","    train_curve = []\n","\n","    best_test_f1 = -1\n","    start_epoch = 1\n","    best_model = None\n","\n","    if load_model == \"BEST\" and os.path.exists(args['best_model_path']):\n","        print(\"Loading best model...\")\n","        checkpoint = torch.load(args['best_model_path']) # add map_location='cpu' ONLY if RAM spike crashes Colab!!\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        model.to(device)\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","        best_test_f1 = checkpoint['best_test_f1']\n","        best_model = model\n","        print(\"Best model loaded!\")\n","    elif load_model == \"CHECKPT\" and os.path.exists(args['model_path']):\n","        print(\"Loading saved model...\")\n","        checkpoint = torch.load(args['model_path']) # add map_location='cpu' ONLY if RAM spike crashes Colab!!\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        model.to(device)\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","        best_test_f1 = checkpoint['best_test_f1']\n","        print(\"Saved model loaded!\")\n","\n","    if load_model != \"BEST\":\n","        for epoch in range(start_epoch, args['epochs'] + 1):\n","            # Each epoch takes 6.5 hours in the simplest GCN diffPool case and 20hrs in the\n","            # GraphSAGE CodeBERT case.\n","            print(\"=====Epoch {}\".format(epoch))\n","            print('Training...')\n","            train(model, device, train_loader, optimizer)\n","            test_perf = eval(model, device, test_loader, evaluator, arr_to_seq = lambda arr: decode_arr_to_seq(arr, idx2vocab))\n","            test_f1 = test_perf[dataset.eval_metric]\n","\n","            if (test_f1 > best_test_f1):\n","                best_test_f1 = test_f1\n","                print(\"Saving best model...\")\n","                torch.save({\n","                  'epoch': epoch,\n","                  'model_state_dict': deepcopy(model.state_dict()),\n","                  'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n","                  'best_test_f1': test_f1\n","                }, args['best_model_path'])\n","                best_model = model\n","            elif epoch % args['checkpoint'] == 0:\n","                print(\"Saving checkpoint model...\")\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': deepcopy(model.state_dict()),\n","                    'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n","                    'best_test_f1': test_f1,\n","                }, args['model_path'])\n","\n","    for input_seed in range(10):\n","        np.random.seed(input_seed)\n","        torch.manual_seed(input_seed)\n","\n","        if device == torch.cuda.is_available():\n","            torch.cuda.manual_seed(input_seed)\n","\n","        valid_and_test = dataset[num_train:]\n","        perm = torch.randperm(len(valid_and_test))+torch.tensor(num_train)\n","        split_idx['valid'] = perm[:num_valid]\n","        split_idx['test'] = perm[num_valid:]\n","\n","        assert(len(split_idx['train']) == num_train)\n","        assert(len(split_idx['valid']) == num_valid)\n","        assert(len(split_idx['test']) == num_test)\n","\n","        valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args['batch_size'], shuffle=False, num_workers = args['num_workers'])\n","        test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args['batch_size'], shuffle=False, num_workers = args['num_workers'])\n","\n","        # train_perf = eval(model, device, train_loader, evaluator, arr_to_seq = lambda arr: decode_arr_to_seq(arr, idx2vocab))\n","        valid_perf = eval(best_model, device, valid_loader, evaluator, arr_to_seq = lambda arr: decode_arr_to_seq(arr, idx2vocab))\n","        test_perf = eval(best_model, device, test_loader, evaluator, arr_to_seq = lambda arr: decode_arr_to_seq(arr, idx2vocab))\n","\n","        # print({'Train': train_perf, 'Validation': valid_perf, 'Test': test_perf})\n","        print({'Validation': valid_perf, 'Test': test_perf})\n","        # train_curve.append(train_perf[dataset.eval_metric])\n","        valid_curve.append(valid_perf[dataset.eval_metric])\n","        test_curve.append(test_perf[dataset.eval_metric])\n","\n","    best_test_epoch = np.argmax(np.array(test_curve))\n","    print('F1')\n","\n","    # best_train = max(train_curve)\n","    print('Best test score: {}'.format(test_curve[best_test_epoch]))\n","    print('Best validation score: {}'.format(valid_curve[best_test_epoch]))\n","\n","    val_scores = torch.tensor(valid_curve).to(device)\n","    test_scores = torch.tensor(test_curve).to(device)\n","\n","    test_avg = torch.mean(test_scores)\n","    test_std = torch.std(test_scores)\n","\n","    val_avg = torch.mean(val_scores)\n","    val_std = torch.std(val_scores)\n","\n","    print('Test avg: {}'.format(test_avg))\n","    print('Test std: {}'.format(test_std))\n","    print('Val avg: {}'.format(val_avg))\n","    print('Val std: {}'.format(val_std))\n","\n","    # Save results\n","    if not args['eval_results_path'] == '':\n","        result_dict = {'Val': valid_curve,\n","                       'Test': test_curve,\n","                       'Test_avg': test_avg.item(),\n","                       'Test_std': test_std.item(),\n","                       'Val_avg': val_avg.item(),\n","                       'Val_std': val_std.item(),\n","                       'Params': num_params\n","                       }\n","        torch.save(result_dict, args['eval_results_path'])\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1702821561103,"user":{"displayName":"Nathanael Ren","userId":"05350445776639853397"},"user_tz":300},"id":"7AAKnF_TKOL5","outputId":"1579a4e9-1e45-400b-e526-c2940a9604af"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'F1'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset.eval_metric"]},{"cell_type":"markdown","metadata":{"id":"w7-ct3-xagJ2"},"source":["# Run"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":415,"referenced_widgets":["27d4c86ff7334cbfa3a58438835fd51b","24f5c9dfdc6c4d4499493bc0f2b3ccad","9c1deb459081402a9248a6f16ca637c0","b9e05088812f44968e46ec89f6edf893","55acf3cd04854aed8c1ffcba20513f97","54d6288dfea440f095704bf77d8e311f","e327a6f4c00047ef9a4c370468127070","9938f7a8d1474991b1482cc6908d05f0","611998c00bbf4391a749afd24dd7d0b1","1465b75004504a66bfcf61a5298c56d2","c046a436268b48239fbdd77aa791c953","9c4bc7d5f66b495dbc62b214f3b70b54","af35fbd016994b47980d0d308d815c6d","3b8f6dd20b5449e6b60800352a55d83f","cb5d49b6873d4d67a22610199827b191","17200fc73a774d5c8cacb92e064ab0c3","0e469c9a48164015ac566c6890124adf","9e67d2c5ce2d4730a08296678accffd4","e03ff268db584a4994ae1582965a46a6","de84c103c32c41629b0614d415dd1f4f","e5992a9236d14e46b07aba31f4038916","1346f114a07a43a59c47e4cd70e4ac88","a9ccafe189044ec797556db97711b020","463ff8970bc44b1990226eab40d24c48","f182f20f01e949e8bbed7816fd82fbb5","0ba1248f3b1948f192d6f7dc74c9b899","cfa2a7ab6dc14c56b994bc522cf439ae","d87ab89182c34f148a7f13e2407ac122","9c267001e9764f2892b3fe4d72b3826f","06dbf70ffaed411b86dd0c2bca3c2803","443ab3455d18434f986702e83058a23b","8cb9cf1eb2804fa2a242db06b80db6c3","f6dad45c9296455c9243a36c0db1b41a","89e1088776d04280b10ad29fe6586555","7a510d9b84ba4f5d9da4b6dcdaa26699","8688a1c397334c7880fa16121cae5d49","af5eaab29e7c4e2ab0000c0b81d89b1f","17e1532d0ce9489da8819a10a9881c5b","68e050295c474f21a9c847afe6f81e98","f0c3b2f4462b494d87a5464d8f1e71c8","8c9e4f074c554a34af9da60a6932a2af","75ddd2b721714b25aa609f1e1984df1b","7d2cca530438463e932d88ca949616d3","fe6e9737f2e44dca8419449dc43af542","85190a98cf53494e90ed672127be21b3","08244dab3a6346be80f20cd3baec870c","6996f0309b1e429387a490fa257f6bdb","ce8f7ee6e2044da6a5e804c12e889b95","b01e196b3169490b90601162644bbcfc","c5a5e07868154431abaefa58cdb6e0ff","5010267f5e94467b9130a671b6e91c41","8115c671e2a648d29b8602b01261ea9e","54a05a42b1ed45eb849773eb67ff28e9","6e46c285e4b1475d8fcfce88dbee8495","be7fb61d6e25464bab46ccf9d2e78f9a","891bb576ff9c49efa55a8ee434cb7fab","d78084f5d2c849d5b39e7fba0c76373d","bd7061e6104542b7a48ab76e3b6d9ad8","eee2e309a10b4aa988d5c440e732ef9a","9c2bb24e47614515a64ec9fd370aae53"]},"id":"3e1hkOh0nJtj","outputId":"7c09b394-8831-4d08-fc45-bd86e73744e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Training\n","{'device': 0, 'num_layer': 3, 'emb_dim': 256, 'drop_ratio': 0, 'batch_size': 64, 'lr': 0.01, 'epochs': 25, 'num_vocab': 5000, 'max_seq_len': 5, 'diff_pool_layers': [(32, 3)], 'max_num_nodes': 1500, 'random_split': False, 'dataset': 'ogbg-code2', 'num_workers': 0, 'best_model_path': '/content/gdrive/MyDrive/CS MS/Colab_Notebooks/CS224W_Final_Project/OGB_submission/best_model_ast_gs_params.pt', 'model_path': '/content/gdrive/MyDrive/CS MS/Colab_Notebooks/CS224W_Final_Project/OGB_submission/model_params.pt', 'eval_results_path': '/content/gdrive/MyDrive/CS MS/Colab_Notebooks/CS224W_Final_Project/OGB_submission/eval_ast_gs_results.pt', 'checkpoint': 5}\n","Target sequence less or equal to 5 is 0.9874166466036873%.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["Coverage of top 5000 vocabulary:\n","0.901200162173439\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["model size: 38.524MB\n","#Params: 10095826\n","Loading best model...\n","Best model loaded!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27d4c86ff7334cbfa3a58438835fd51b","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c4bc7d5f66b495dbc62b214f3b70b54","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.18033849030693486, 'recall': 0.12436970902369507, 'F1': 0.13966598915102224}, 'Test': {'precision': 0.18069224226960692, 'recall': 0.1261394356357451, 'F1': 0.14097376890569893}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9ccafe189044ec797556db97711b020","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89e1088776d04280b10ad29fe6586555","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.17936699244715198, 'recall': 0.12476832576100666, 'F1': 0.13964612788077746}, 'Test': {'precision': 0.18170220521232003, 'recall': 0.12572503623282738, 'F1': 0.1409944165548321}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85190a98cf53494e90ed672127be21b3","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08244dab3a6346be80f20cd3baec870c","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.17717564389125068, 'recall': 0.12417238330373274, 'F1': 0.13872823110233812}, 'Test': {'precision': 0.18398031711317658, 'recall': 0.12634457418834125, 'F1': 0.14194865615724006}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6996f0309b1e429387a490fa257f6bdb","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce8f7ee6e2044da6a5e804c12e889b95","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.18221209332223048, 'recall': 0.1264398899177792, 'F1': 0.141592911819059}, 'Test': {'precision': 0.1787444565943746, 'recall': 0.12398728885880334, 'F1': 0.13897055246153114}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b01e196b3169490b90601162644bbcfc","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5a5e07868154431abaefa58cdb6e0ff","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.18340126513856625, 'recall': 0.12720244443192244, 'F1': 0.14255227346562752}, 'Test': {'precision': 0.17750820120284308, 'recall': 0.12319454207781162, 'F1': 0.1379732062482199}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5010267f5e94467b9130a671b6e91c41","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8115c671e2a648d29b8602b01261ea9e","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.18079136900848783, 'recall': 0.12554458489945186, 'F1': 0.1408604828138509}, 'Test': {'precision': 0.18022143247676325, 'recall': 0.1249180422123287, 'F1': 0.1397319809566939}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54a05a42b1ed45eb849773eb67ff28e9","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e46c285e4b1475d8fcfce88dbee8495","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.1812625089480066, 'recall': 0.1252395874421119, 'F1': 0.14043842528744138}, 'Test': {'precision': 0.1797316384180791, 'recall': 0.1252351156123709, 'F1': 0.1401707492535814}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be7fb61d6e25464bab46ccf9d2e78f9a","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"891bb576ff9c49efa55a8ee434cb7fab","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.18057953864808401, 'recall': 0.12451275538776087, 'F1': 0.1397919458422592}, 'Test': {'precision': 0.18044164996051273, 'recall': 0.12599072555332808, 'F1': 0.1408428251375217}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d78084f5d2c849d5b39e7fba0c76373d","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd7061e6104542b7a48ab76e3b6d9ad8","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.18043564009291319, 'recall': 0.12513424618653174, 'F1': 0.14027188279719285}, 'Test': {'precision': 0.18059124597533566, 'recall': 0.1253446277058907, 'F1': 0.1403438857580465}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eee2e309a10b4aa988d5c440e732ef9a","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/357 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c2bb24e47614515a64ec9fd370aae53","version_major":2,"version_minor":0},"text/plain":["Iteration:   0%|          | 0/343 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'Validation': {'precision': 0.18174314473126765, 'recall': 0.12674106121097398, 'F1': 0.14175639654011044}, 'Test': {'precision': 0.17923197254115786, 'recall': 0.12367419311446103, 'F1': 0.1388005947943072}}\n","F1\n","Best test score: 0.14194865615724006\n","Best validation score: 0.13872823110233812\n","Test avg: 0.14007506362276728\n","Test std: 0.0012116946189855466\n","Val avg: 0.1405304666699679\n","Val std: 0.0011655464564795897\n"]}],"source":["# Takes roughly 6 hours to train for AST encoder, 20 for CodeBERT\n","# \"BEST\" to load best model, \"CHECKPT\" to load checkpoint aka where you stopped training, or \"OVERWRITE\" for overwriting everything\n","main(\"OVERWRITE\")"]},{"cell_type":"markdown","metadata":{"id":"aTqC6Q1QWxuN"},"source":["# Some random exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6WvYngWbjE0"},"outputs":[],"source":["import networkx as nx\n","import torch_geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UX11FbYG-8j3"},"outputs":[],"source":["print(len(train_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5896zPkv-_hw"},"outputs":[],"source":["print(len(valid_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aTPtRfz_Bna"},"outputs":[],"source":["print(len(test_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjlacyXEYlJ-"},"outputs":[],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaTPJMZSZ0zN"},"outputs":[],"source":["G = torch_geometric.utils.to_networkx(dataset[0])\n","nx.draw(G, with_labels = True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaMSUeiAaYox"},"outputs":[],"source":["print(G.number_of_edges(), G.number_of_nodes())\n","G.number_of_edges() / G.number_of_nodes()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIWbOF76fhr2"},"outputs":[],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1B9YQ2GJb23t"},"outputs":[],"source":["for d in dataset[:1]:\n","  for pair in zip(d.x, d.node_is_attributed):\n","    print(pair)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bZ6Hvv9bd2gG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["6bo9YEJuQ6P4","aTqC6Q1QWxuN"],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"06dbf70ffaed411b86dd0c2bca3c2803":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ba1248f3b1948f192d6f7dc74c9b899":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cb9cf1eb2804fa2a242db06b80db6c3","placeholder":"​","style":"IPY_MODEL_f6dad45c9296455c9243a36c0db1b41a","value":" 357/357 [01:16&lt;00:00,  5.42it/s]"}},"0e469c9a48164015ac566c6890124adf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1346f114a07a43a59c47e4cd70e4ac88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1465b75004504a66bfcf61a5298c56d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17200fc73a774d5c8cacb92e064ab0c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17e1532d0ce9489da8819a10a9881c5b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24f5c9dfdc6c4d4499493bc0f2b3ccad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54d6288dfea440f095704bf77d8e311f","placeholder":"​","style":"IPY_MODEL_e327a6f4c00047ef9a4c370468127070","value":"Iteration: 100%"}},"27d4c86ff7334cbfa3a58438835fd51b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24f5c9dfdc6c4d4499493bc0f2b3ccad","IPY_MODEL_9c1deb459081402a9248a6f16ca637c0","IPY_MODEL_b9e05088812f44968e46ec89f6edf893"],"layout":"IPY_MODEL_55acf3cd04854aed8c1ffcba20513f97"}},"3b8f6dd20b5449e6b60800352a55d83f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e03ff268db584a4994ae1582965a46a6","max":343,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de84c103c32c41629b0614d415dd1f4f","value":343}},"443ab3455d18434f986702e83058a23b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"463ff8970bc44b1990226eab40d24c48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d87ab89182c34f148a7f13e2407ac122","placeholder":"​","style":"IPY_MODEL_9c267001e9764f2892b3fe4d72b3826f","value":"Iteration: 100%"}},"54d6288dfea440f095704bf77d8e311f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55acf3cd04854aed8c1ffcba20513f97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"611998c00bbf4391a749afd24dd7d0b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68e050295c474f21a9c847afe6f81e98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75ddd2b721714b25aa609f1e1984df1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a510d9b84ba4f5d9da4b6dcdaa26699":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68e050295c474f21a9c847afe6f81e98","placeholder":"​","style":"IPY_MODEL_f0c3b2f4462b494d87a5464d8f1e71c8","value":"Iteration:   8%"}},"7d2cca530438463e932d88ca949616d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8688a1c397334c7880fa16121cae5d49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c9e4f074c554a34af9da60a6932a2af","max":343,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75ddd2b721714b25aa609f1e1984df1b","value":27}},"89e1088776d04280b10ad29fe6586555":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a510d9b84ba4f5d9da4b6dcdaa26699","IPY_MODEL_8688a1c397334c7880fa16121cae5d49","IPY_MODEL_af5eaab29e7c4e2ab0000c0b81d89b1f"],"layout":"IPY_MODEL_17e1532d0ce9489da8819a10a9881c5b"}},"8c9e4f074c554a34af9da60a6932a2af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cb9cf1eb2804fa2a242db06b80db6c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9938f7a8d1474991b1482cc6908d05f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c1deb459081402a9248a6f16ca637c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9938f7a8d1474991b1482cc6908d05f0","max":357,"min":0,"orientation":"horizontal","style":"IPY_MODEL_611998c00bbf4391a749afd24dd7d0b1","value":357}},"9c267001e9764f2892b3fe4d72b3826f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c4bc7d5f66b495dbc62b214f3b70b54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af35fbd016994b47980d0d308d815c6d","IPY_MODEL_3b8f6dd20b5449e6b60800352a55d83f","IPY_MODEL_cb5d49b6873d4d67a22610199827b191"],"layout":"IPY_MODEL_17200fc73a774d5c8cacb92e064ab0c3"}},"9e67d2c5ce2d4730a08296678accffd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9ccafe189044ec797556db97711b020":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_463ff8970bc44b1990226eab40d24c48","IPY_MODEL_f182f20f01e949e8bbed7816fd82fbb5","IPY_MODEL_0ba1248f3b1948f192d6f7dc74c9b899"],"layout":"IPY_MODEL_cfa2a7ab6dc14c56b994bc522cf439ae"}},"af35fbd016994b47980d0d308d815c6d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e469c9a48164015ac566c6890124adf","placeholder":"​","style":"IPY_MODEL_9e67d2c5ce2d4730a08296678accffd4","value":"Iteration: 100%"}},"af5eaab29e7c4e2ab0000c0b81d89b1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d2cca530438463e932d88ca949616d3","placeholder":"​","style":"IPY_MODEL_fe6e9737f2e44dca8419449dc43af542","value":" 27/343 [00:05&lt;01:07,  4.65it/s]"}},"b9e05088812f44968e46ec89f6edf893":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1465b75004504a66bfcf61a5298c56d2","placeholder":"​","style":"IPY_MODEL_c046a436268b48239fbdd77aa791c953","value":" 357/357 [01:18&lt;00:00,  5.50it/s]"}},"c046a436268b48239fbdd77aa791c953":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb5d49b6873d4d67a22610199827b191":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5992a9236d14e46b07aba31f4038916","placeholder":"​","style":"IPY_MODEL_1346f114a07a43a59c47e4cd70e4ac88","value":" 343/343 [01:13&lt;00:00,  4.73it/s]"}},"cfa2a7ab6dc14c56b994bc522cf439ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d87ab89182c34f148a7f13e2407ac122":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de84c103c32c41629b0614d415dd1f4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e03ff268db584a4994ae1582965a46a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e327a6f4c00047ef9a4c370468127070":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5992a9236d14e46b07aba31f4038916":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0c3b2f4462b494d87a5464d8f1e71c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f182f20f01e949e8bbed7816fd82fbb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06dbf70ffaed411b86dd0c2bca3c2803","max":357,"min":0,"orientation":"horizontal","style":"IPY_MODEL_443ab3455d18434f986702e83058a23b","value":357}},"f6dad45c9296455c9243a36c0db1b41a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe6e9737f2e44dca8419449dc43af542":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}